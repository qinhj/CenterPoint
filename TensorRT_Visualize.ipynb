{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "777b8e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No APEX!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import apex\n",
    "except:\n",
    "    print(\"No APEX!\")\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import yaml\n",
    "from det3d import torchie\n",
    "from det3d.datasets import build_dataloader, build_dataset\n",
    "from det3d.models import build_detector\n",
    "from det3d.torchie import Config\n",
    "from det3d.torchie.apis import (\n",
    "    batch_processor,\n",
    "    build_optimizer,\n",
    "    get_root_logger,\n",
    "    init_dist,\n",
    "    set_random_seed,\n",
    "    train_detector,\n",
    "    example_to_device,\n",
    ")\n",
    "from det3d.torchie.trainer import load_checkpoint\n",
    "import pickle \n",
    "import time \n",
    "from matplotlib import pyplot as plt \n",
    "from det3d.torchie.parallel import collate, collate_kitti\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.cm as cm\n",
    "import subprocess\n",
    "import cv2\n",
    "from tools.demo_utils import visual \n",
    "from collections import defaultdict\n",
    "from det3d.torchie.trainer.utils import all_gather, synchronize\n",
    "from pathlib import PosixPath\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b4efa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_box(info):\n",
    "    boxes =  info[\"gt_boxes\"].astype(np.float32)\n",
    "    names = info[\"gt_names\"]\n",
    "\n",
    "    assert len(boxes) == len(names)\n",
    "\n",
    "    detection = {}\n",
    "\n",
    "    detection['box3d_lidar'] = boxes\n",
    "\n",
    "    # dummy value \n",
    "    detection['label_preds'] = np.zeros(len(boxes)) \n",
    "    detection['scores'] = np.ones(len(boxes))\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36357bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use HM Bias:  -2.19\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('configs/nusc/pp/nusc_centerpoint_pp_02voxel_two_pfn_10sweep_demo_mini.py')\n",
    "\n",
    "model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n",
    "\n",
    "dataset = build_dataset(cfg.data.val)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    sampler=None,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_kitti,\n",
    "    pin_memory=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4fa1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint(model, './latest.pth', map_location=\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "gpu_device = torch.device(\"cuda\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "points_list = [] \n",
    "gt_annos = [] \n",
    "detections = {}\n",
    "detections_for_draw = []\n",
    "points_list = []\n",
    "token_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6bdc9",
   "metadata": {},
   "source": [
    "# Inference on nuScenes Mini dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9352ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.3 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Finish generate predictions for testset, save to infos_val_10sweeps_withvelo_filter_True.json\n",
      "Initializing nuScenes detection evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 476.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded results from infos_val_10sweeps_withvelo_filter_True.json. Found detections for 81 samples.\n",
      "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
      "Loaded ground truth annotations for 81 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering predictions\n",
      "=> Original number of boxes: 17655\n",
      "=> After distance based filtering: 14562\n",
      "=> After LIDAR points based filtering: 14562\n",
      "=> After bike rack filtering: 14493\n",
      "Filtering ground truth annotations\n",
      "=> Original number of boxes: 4441\n",
      "=> After distance based filtering: 3785\n",
      "=> After LIDAR points based filtering: 3393\n",
      "=> After bike rack filtering: 3393\n",
      "Rendering sample token b6c420c3a5bd4a219b1cb82ee5ea0aa7\n",
      "Rendering sample token b22fa0b3c34f47b6a360b60f35d5d567\n",
      "Rendering sample token d8251bbc2105497ab8ec80827d4429aa\n",
      "Rendering sample token 372725a4b00e49c78d6d0b1c4a38b6e0\n",
      "Rendering sample token ce94ef7a0522468e81c0e2b3a2f1e12d\n",
      "Rendering sample token 0d0700a2284e477db876c3ee1d864668\n",
      "Rendering sample token 61a7bd24f88a46c2963280d8b13ac675\n",
      "Rendering sample token fa65a298c01f44e7a182bbf9e5fe3697\n",
      "Rendering sample token 8573a885a7cb41d185c05029eeb9a54e\n",
      "Rendering sample token 38a28a3aaf2647f2a8c0e90e31267bf8\n",
      "Accumulating metric data...\n",
      "Calculating metrics...\n",
      "Rendering PR and TP curves\n",
      "Saving metrics to: ./\n",
      "mAP: 0.4163\n",
      "mATE: 0.4438\n",
      "mASE: 0.4516\n",
      "mAOE: 0.5674\n",
      "mAVE: 0.4429\n",
      "mAAE: 0.3288\n",
      "NDS: 0.4847\n",
      "Eval time: 3.4s\n",
      "\n",
      "Per-class results:\n",
      "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
      "car\t0.887\t0.189\t0.161\t0.233\t0.129\t0.082\n",
      "truck\t0.668\t0.152\t0.182\t0.379\t0.115\t0.079\n",
      "bus\t0.967\t0.227\t0.161\t0.024\t0.622\t0.336\n",
      "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
      "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
      "pedestrian\t0.886\t0.160\t0.251\t0.395\t0.207\t0.130\n",
      "motorcycle\t0.510\t0.242\t0.265\t0.805\t0.051\t0.003\n",
      "bicycle\t0.213\t0.297\t0.190\t0.272\t0.421\t0.000\n",
      "traffic_cone\t0.032\t0.172\t0.307\tnan\tnan\tnan\n",
      "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
      "Evaluation nusc: Nusc v1.0-mini Evaluation\n",
      "car Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "77.77, 89.54, 92.78, 94.72 mean AP: 0.8870277030460021\n",
      "truck Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "64.08, 65.91, 67.30, 70.00 mean AP: 0.6682436991249997\n",
      "construction_vehicle Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "0.00, 0.00, 0.00, 0.00 mean AP: 0.0\n",
      "bus Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "88.95, 99.23, 99.23, 99.23 mean AP: 0.9665763655287005\n",
      "trailer Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "0.00, 0.00, 0.00, 0.00 mean AP: 0.0\n",
      "barrier Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "0.00, 0.00, 0.00, 0.00 mean AP: 0.0\n",
      "motorcycle Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "45.24, 50.58, 53.49, 54.88 mean AP: 0.510455619281946\n",
      "bicycle Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "21.26, 21.26, 21.26, 21.26 mean AP: 0.21263511704651997\n",
      "pedestrian Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "84.86, 88.00, 89.72, 91.92 mean AP: 0.8862602743260951\n",
      "traffic_cone Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "3.22, 3.22, 3.22, 3.22 mean AP: 0.03224481049590753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, data_batch in enumerate(data_loader):\n",
    "    token = data_batch['metadata'][0]['token']\n",
    "    token_list.append(token)\n",
    "    \n",
    "    # save points data for tensorrt\n",
    "    data_batch[\"points\"].cpu().numpy()[:,1:].astype(np.float32).tofile( \\\n",
    "                      \"./tensorrt/data/centerpoint/points/%s.bin\"%token)\n",
    "    \n",
    "    # points_list for visulize\n",
    "    points = data_batch['points'][:, 1:4].cpu().numpy()\n",
    "    points_list.append(points.T)\n",
    "    with torch.no_grad():\n",
    "        outputs = batch_processor(\n",
    "            model, data_batch, train_mode=False, local_rank=0\n",
    "        )\n",
    "    info = dataset._nusc_infos[i]\n",
    "    gt_annos.append(convert_box(info))\n",
    "    \n",
    "    for output in outputs:\n",
    "        token = output[\"metadata\"][\"token\"]\n",
    "        for k, v in output.items():\n",
    "            if k not in [\n",
    "                \"metadata\",\n",
    "            ]:\n",
    "                output[k] = v.to(cpu_device)\n",
    "        detections_for_draw.append(output)\n",
    "        detections.update(\n",
    "            {token: output,}\n",
    "        )\n",
    "\n",
    "all_predictions = all_gather(detections)\n",
    "\n",
    "predictions = {}\n",
    "for p in all_predictions:\n",
    "    predictions.update(p)\n",
    "\n",
    "result_dict, _ = dataset.evaluation(copy.deepcopy(predictions), output_dir=\"./\", testset=False)\n",
    "\n",
    "if result_dict is not None:\n",
    "    for k, v in result_dict[\"results\"].items():\n",
    "        print(f\"Evaluation {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463c164",
   "metadata": {},
   "source": [
    "# Visualize Pytorch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90a53fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model inference. Please wait a minute, the matplotlib is a little slow...\n",
      "Rendered Image 0\n",
      "Rendered Image 1\n",
      "Rendered Image 2\n",
      "Rendered Image 3\n",
      "Rendered Image 4\n",
      "Rendered Image 5\n",
      "Rendered Image 6\n",
      "Rendered Image 7\n",
      "Rendered Image 8\n",
      "Rendered Image 9\n"
     ]
    }
   ],
   "source": [
    "print('Done model inference. Please wait a minute, the matplotlib is a little slow...')\n",
    "\n",
    "vis_num = 10\n",
    "draw_num = min(vis_num, len(points_list))\n",
    "for i in range(draw_num):\n",
    "    visual(points_list[i], gt_annos[i], detections_for_draw[i], i, save_path=\"demo/torch_demo\")\n",
    "    print(\"Rendered Image {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02335046",
   "metadata": {},
   "source": [
    "# Evaluete TensorRT Result\n",
    "1. copy the ./tensorrt/data/centerpoint/points to <TensorRT root directory\\>/data/centerpoint\n",
    "2. run the <TensorRT root directory\\>/bin/centerpoint to get the tensorrt outputs.\n",
    "3. copy the <TensorRT root directory\\>/data/centerpoint back the CenterPoint/tensorrt/data\n",
    "4. run the following python code to do evaluation and visualiza tensorrt result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2159f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trt_result(path):\n",
    "    token = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    trt_pred = {} \n",
    "    with open(path) as f:\n",
    "        trt_res = f.readlines()\n",
    "\n",
    "    boxs = []\n",
    "    box3d = []\n",
    "    score = []\n",
    "    cls = []\n",
    "    for line in trt_res:\n",
    "        box3d += [np.array([float(it) for it in line.strip().split(\" \")[:9]])]\n",
    "        score += [np.array([float(line.strip().split(\" \")[-2])])]\n",
    "        cls += [np.array([int(line.strip().split(\" \")[-1])])]\n",
    "\n",
    "    trt_pred[\"box3d_lidar\"] = torch.from_numpy(np.array(box3d))\n",
    "    trt_pred[\"scores\"] = torch.from_numpy(np.array(score))\n",
    "    trt_pred[\"label_preds\"] = torch.from_numpy(np.array(cls,np.int32))\n",
    "    trt_pred[\"metadata\"] = {}\n",
    "\n",
    "    trt_pred[\"metadata\"][\"image_prefix\"] = PosixPath('/home/dean/dataset/nuscenes')\n",
    "    trt_pred[\"metadata\"][\"num_point_features\"] = 5\n",
    "    trt_pred[\"metadata\"][\"token\"] = token\n",
    "    \n",
    "    return trt_pred, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d7e3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dict = {}\n",
    "points_list = []\n",
    "gt_annos_dict =  {}\n",
    "for i, data_batch in enumerate(data_loader):\n",
    "    token = data_batch['metadata'][0]['token']\n",
    "    points = data_batch['points'][:, 1:4].cpu().numpy()\n",
    "    points_dict[token] = points.T\n",
    "    \n",
    "    info = dataset._nusc_infos[i]\n",
    "    gt_annos_dict[token] = convert_box(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd0d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84f206d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.3 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Finish generate predictions for testset, save to infos_val_10sweeps_withvelo_filter_True.json\n",
      "Initializing nuScenes detection evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 484.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded results from infos_val_10sweeps_withvelo_filter_True.json. Found detections for 81 samples.\n",
      "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n",
      "Loaded ground truth annotations for 81 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering predictions\n",
      "=> Original number of boxes: 15816\n",
      "=> After distance based filtering: 12879\n",
      "=> After LIDAR points based filtering: 12879\n",
      "=> After bike rack filtering: 12820\n",
      "Filtering ground truth annotations\n",
      "=> Original number of boxes: 4441\n",
      "=> After distance based filtering: 3785\n",
      "=> After LIDAR points based filtering: 3393\n",
      "=> After bike rack filtering: 3393\n",
      "Rendering sample token b6c420c3a5bd4a219b1cb82ee5ea0aa7\n",
      "Rendering sample token b22fa0b3c34f47b6a360b60f35d5d567\n",
      "Rendering sample token d8251bbc2105497ab8ec80827d4429aa\n",
      "Rendering sample token 372725a4b00e49c78d6d0b1c4a38b6e0\n",
      "Rendering sample token ce94ef7a0522468e81c0e2b3a2f1e12d\n",
      "Rendering sample token 0d0700a2284e477db876c3ee1d864668\n",
      "Rendering sample token 61a7bd24f88a46c2963280d8b13ac675\n",
      "Rendering sample token fa65a298c01f44e7a182bbf9e5fe3697\n",
      "Rendering sample token 8573a885a7cb41d185c05029eeb9a54e\n",
      "Rendering sample token 38a28a3aaf2647f2a8c0e90e31267bf8\n",
      "Accumulating metric data...\n",
      "Calculating metrics...\n",
      "Rendering PR and TP curves\n",
      "Saving metrics to: ./\n",
      "mAP: 0.4007\n",
      "mATE: 0.4433\n",
      "mASE: 0.4537\n",
      "mAOE: 0.5665\n",
      "mAVE: 0.4416\n",
      "mAAE: 0.3191\n",
      "NDS: 0.4779\n",
      "Eval time: 3.3s\n",
      "\n",
      "Per-class results:\n",
      "Object Class\tAP\tATE\tASE\tAOE\tAVE\tAAE\n",
      "car\t0.878\t0.189\t0.161\t0.227\t0.130\t0.075\n",
      "truck\t0.664\t0.155\t0.181\t0.356\t0.107\t0.066\n",
      "bus\t0.965\t0.235\t0.163\t0.024\t0.607\t0.287\n",
      "trailer\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
      "construction_vehicle\t0.000\t1.000\t1.000\t1.000\t1.000\t1.000\n",
      "pedestrian\t0.795\t0.145\t0.247\t0.386\t0.216\t0.121\n",
      "motorcycle\t0.470\t0.226\t0.264\t0.801\t0.049\t0.004\n",
      "bicycle\t0.206\t0.253\t0.188\t0.303\t0.423\t0.000\n",
      "traffic_cone\t0.029\t0.230\t0.332\tnan\tnan\tnan\n",
      "barrier\t0.000\t1.000\t1.000\t1.000\tnan\tnan\n",
      "Evaluation nusc: Nusc v1.0-mini Evaluation\n",
      "car Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "77.02, 88.66, 91.76, 93.71 mean AP: 0.8778580903157415\n",
      "truck Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "63.76, 65.52, 66.93, 69.56 mean AP: 0.6644005931411525\n",
      "construction_vehicle Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "0.00, 0.00, 0.00, 0.00 mean AP: 0.0\n",
      "bus Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "88.12, 99.23, 99.23, 99.23 mean AP: 0.964554530775544\n",
      "trailer Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "0.00, 0.00, 0.00, 0.00 mean AP: 0.0\n",
      "barrier Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "0.00, 0.00, 0.00, 0.00 mean AP: 0.0\n",
      "motorcycle Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "41.82, 46.70, 49.18, 50.35 mean AP: 0.47015238202322285\n",
      "bicycle Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "20.62, 20.62, 20.62, 20.62 mean AP: 0.20618365632217506\n",
      "pedestrian Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "74.13, 76.30, 80.51, 86.95 mean AP: 0.7947296778889967\n",
      "traffic_cone Nusc dist AP@0.5, 1.0, 2.0, 4.0\n",
      "2.89, 2.89, 2.97, 2.97 mean AP: 0.02927880420347778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trt_pred = {}\n",
    "detections = {}\n",
    "detections_for_draw = []\n",
    "gt_annos = []\n",
    "res_path_list = glob.glob(\"./tensorrt/data/centerpoint/results/*.txt\")\n",
    "output_dict = {}\n",
    "\n",
    "for path in res_path_list:\n",
    "    output, token = read_trt_result(path)\n",
    "    output_dict[token] = output\n",
    "\n",
    "for token in token_list:    \n",
    "    points_list.append(points_dict[token])\n",
    "    gt_annos.append(gt_annos_dict[token])\n",
    "    output = output_dict[token]\n",
    "    for k, v in output.items():\n",
    "        if k not in [\n",
    "            \"metadata\",\n",
    "        ]:\n",
    "            output[k] = v\n",
    "    detections_for_draw.append(output)\n",
    "    detections.update(\n",
    "        {token: output,}\n",
    "    )\n",
    "all_predictions = all_gather(detections)\n",
    "\n",
    "predictions = {}\n",
    "for p in all_predictions:\n",
    "    predictions.update(p)\n",
    "\n",
    "result_dict, _ = dataset.evaluation(copy.deepcopy(predictions), output_dir=\"./\", testset=False)\n",
    "\n",
    "if result_dict is not None:\n",
    "    for k, v in result_dict[\"results\"].items():\n",
    "        print(f\"Evaluation {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c99909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9ae876",
   "metadata": {},
   "source": [
    "# Visualize TensorRT Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6707c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model inference. Please wait a minute, the matplotlib is a little slow...\n",
      "Rendered Image 0\n",
      "Rendered Image 1\n",
      "Rendered Image 2\n",
      "Rendered Image 3\n",
      "Rendered Image 4\n",
      "Rendered Image 5\n",
      "Rendered Image 6\n",
      "Rendered Image 7\n",
      "Rendered Image 8\n",
      "Rendered Image 9\n"
     ]
    }
   ],
   "source": [
    "print('Done model inference. Please wait a minute, the matplotlib is a little slow...')\n",
    "vis_num = 10\n",
    "draw_num = min(vis_num, len(points_list))\n",
    "for i in range(draw_num):\n",
    "    visual(points_list[i], gt_annos[i], detections_for_draw[i], i, save_path=\"demo/trt_demo\")\n",
    "    print(\"Rendered Image {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d00dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080862c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7310d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ad5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0ec16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
